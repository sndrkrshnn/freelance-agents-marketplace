name: Database Backup

on:
  # Daily backup at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Backup type'
        required: false
        default: 'daily'
        type: choice
        options:
          - daily
          - weekly
          - monthly
      
      upload_to_s3:
        description: 'Upload to S3'
        required: false
        default: false
        type: boolean
      
      verify_backup:
        description: 'Verify backup integrity'
        required: false
        default: true
        type: boolean

  # Also run on pushes to main (optional, for testing)
  push:
    branches:
      - main
    paths:
      - 'scripts/**'
      - '.github/workflows/database-backup.yml'

env:
  NODE_ENV: production
  # Override with secrets if available
  DB_HOST: ${{ secrets.DB_HOST || 'localhost' }}
  DB_PORT: ${{ secrets.DB_PORT || '5432' }}
  DB_NAME: ${{ secrets.DB_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DATABASE_URL: ${{ secrets.DATABASE_URL }}

jobs:
  backup:
    name: Database Backup
    runs-on: ubuntu-latest
    
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client postgresql-common gzip
      
      - name: Setup AWS CLI (optional)
        if: env.AWS_S3_BUCKET != ''
        run: |
          sudo apt-get install -y awscli
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET || '' }}

      - name: Create backup directories
        run: |
          mkdir -p ${{ github.workspace }}/backups/{daily,weekly,monthly}
          mkdir -p ${{ github.workspace }}/logs
          mkdir -p ${{ github.workspace }}/tmp

      - name: Set environment variables
        run: |
          echo "BACKUP_DIR=${{ github.workspace }}/backups" >> $GITHUB_ENV
          echo "LOG_DIR=${{ github.workspace }}/logs" >> $GITHUB_ENV
          echo "TEMP_DIR=${{ github.workspace }}/tmp" >> $GITHUB_ENV
          echo "BACKUP_RETENTION_DAYS=7" >> $GITHUB_ENV
          echo "BACKUP_RETENTION_WEEKS=4" >> $GITHUB_ENV
          echo "BACKUP_RETENTION_MONTHS=12" >> $GITHUB_ENV
          
          # Set database password
          if [ -n "${{ secrets.DATABASE_URL }}" ]; then
            export PGPASSWORD=$(echo ${{ secrets.DATABASE_URL }} | sed -n 's/^.*:\([^@]*\)@.*$/\1/p')
            echo "PGPASSWORD=${PGPASSWORD}" >> $GITHUB_ENV
          elif [ -n "${{ secrets.DB_PASSWORD }}" ]; then
            echo "PGPASSWORD=${{ secrets.DB_PASSWORD }}" >> $GITHUB_ENV
          fi
          
          # Set S3 configuration
          if [ -n "${{ secrets.AWS_S3_BUCKET }}" ]; then
            echo "AWS_S3_BUCKET=${{ secrets.AWS_S3_BUCKET }}" >> $GITHUB_ENV
            echo "AWS_S3_REGION=${{ secrets.AWS_S3_REGION || 'us-east-1' }}" >> $GITHUB_ENV
          fi
          
          # Set notification configuration
          if [ -n "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            echo "SLACK_WEBHOOK_URL=${{ secrets.SLACK_WEBHOOK_URL }}" >> $GITHUB_ENV
          fi
          if [ -n "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "DISCORD_WEBHOOK_URL=${{ secrets.DISCORD_WEBHOOK_URL }}" >> $GITHUB_ENV
          fi

      - name: Test database connection
        run: |
          psql -h ${{ secrets.DB_HOST || 'localhost' }} \
               -p ${{ secrets.DB_PORT || '5432' }} \
               -U ${{ secrets.DB_USER }} \
               -d postgres \
               -c "SELECT version();"
          
          # Check if target database exists
          psql -h ${{ secrets.DB_HOST || 'localhost' }} \
               -p ${{ secrets.DB_PORT || '5432' }} \
               -U ${{ secrets.DB_USER }} \
               -d postgres \
               -lqt | cut -d \| -f 1 | grep -qw ${{ secrets.DB_NAME }}

      - name: Create database backup
        id: backup
        run: |
          TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
          BACKUP_FILE="${{ env.BACKUP_DIR }}/daily/freelance_agents_db_${TIMESTAMP}.sql.gz"
          
          echo "Creating backup: ${BACKUP_FILE}"
          
          # Create dump
          pg_dump -h ${{ secrets.DB_HOST || 'localhost' }} \
                  -p ${{ secrets.DB_PORT || '5432' }} \
                  -U ${{ secrets.DB_USER }} \
                  -d ${{ secrets.DB_NAME }} \
                  -F c \
                  -f /tmp/backup.dump \
                  -v \
                  -j 4
          
          # Compress
          gzip -6 -c /tmp/backup.dump > "${BACKUP_FILE}"
          rm -f /tmp/backup.dump
          
          # Get file size
          FILE_SIZE=$(du -h "${BACKUP_FILE}" | cut -f1)
          
          echo "backup_file=${BACKUP_FILE}" >> $GITHUB_OUTPUT
          echo "backup_size=${FILE_SIZE}" >> $GITHUB_OUTPUT
          echo "Backup created successfully: ${FILE_SIZE}"

      - name: Verify backup integrity
        if: github.event.inputs.verify_backup != 'false'
        run: |
          echo "Verifying backup: ${{ steps.backup.outputs.backup_file }}"
          
          # Test gzip integrity
          if ! gzip -t "${{ steps.backup.outputs.backup_file }}"; then
            echo "ERROR: Backup integrity check failed"
            exit 1
          fi
          
          echo "Backup integrity check passed"
          
          # Get database size comparison
          DB_SIZE=$(psql -h ${{ secrets.DB_HOST || 'localhost' }} \
                         -p ${{ secrets.DB_PORT || '5432' }} \
                         -U ${{ secrets.DB_USER }} \
                         -d postgres \
                         -t -c "SELECT pg_size_pretty(pg_database_size('${{ secrets.DB_NAME }}'))" | xargs)
          
          echo "Database size: ${DB_SIZE}"
          echo "Backup size: ${{ steps.backup.outputs.backup_size }}"

      - name: Test restore to temp database
        id: test_restore
        if: github.event.inputs.verify_backup != 'false'
        run: |
          TEMP_DB="verify_test_${GITHUB_RUN_ID}"
          
          echo "Creating temporary database: ${TEMP_DB}"
          
          # Create temp database
          psql -h ${{ secrets.DB_HOST || 'localhost' }} \
               -p ${{ secrets.DB_PORT || '5432' }} \
               -U ${{ secrets.DB_USER }} \
               -d postgres \
               -c "CREATE DATABASE ${TEMP_DB};"
          
          # Test restore
          pg_restore -h ${{ secrets.DB_HOST || 'localhost' }} \
                    -p ${{ secrets.DB_PORT || '5432' }} \
                    -U ${{ secrets.DB_USER }} \
                    -d ${TEMP_DB} \
                    -v \
                    -j 4 \
                    "${{ steps.backup.outputs.backup_file }}"
          
          # Check restored tables
          TABLE_COUNT=$(psql -h ${{ secrets.DB_HOST || 'localhost' }} \
                             -p ${{ secrets.DB_PORT || '5432' }} \
                             -U ${{ secrets.DB_USER }} \
                             -d ${TEMP_DB} \
                             -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';")
          
          echo "Restored ${TABLE_COUNT} tables"
          
          # Drop temp database
          psql -h ${{ secrets.DB_HOST || 'localhost' }} \
               -p ${{ secrets.DB_PORT || '5432' }} \
               -U ${{ secrets.DB_USER }} \
               -d postgres \
               -c "DROP DATABASE ${TEMP_DB};"
          
          echo "Test restore successful"

      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ github.run_number }}
          path: ${{ steps.backup.outputs.backup_file }}
          retention-days: 7
      
      - name: Upload backup logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backup-logs-${{ github.run_number }}
          path: ${{ env.LOG_DIR }}/
          retention-days: 7

      - name: Upload to S3
        if: |
          env.AWS_S3_BUCKET != '' &&
          (github.event.inputs.upload_to_s3 == 'true' || github.event_name == 'schedule')
        run: |
          TIMESTAMP=$(date '+%Y-%m-%d')
          S3_PATH="s3://${{ secrets.AWS_S3_BUCKET }}/database-backups/${TIMESTAMP}/"
          
          echo "Uploading to S3: ${S3_PATH}"
          
          # Configure AWS credentials
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region ${{ secrets.AWS_S3_REGION || 'us-east-1' }}
          
          # Upload backup
          aws s3 cp "${{ steps.backup.outputs.backup_file }}" \
                   "${S3_PATH}freelance_agents_db_${TIMESTAMP}.sql.gz" \
                   --storage-class STANDARD_IA \
                   --server-side-encryption AES256
          
          echo "Backup uploaded to S3 successfully"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_REGION: ${{ secrets.AWS_S3_REGION || 'us-east-1' }}

      - name: Send success notification to Slack
        if: success() && env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -s -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H 'Content-Type: application/json' \
            -d '{
              "attachments": [
                {
                  "color": "#36a64f",
                  "title": "✓ Database Backup Successful",
                  "fields": [
                    {
                      "title": "Database",
                      "value": "${{ secrets.DB_NAME }}",
                      "short": true
                    },
                    {
                      "title": "Size",
                      "value": "${{ steps.backup.outputs.backup_size }}",
                      "short": true
                    },
                    {
                      "title": "Workflow",
                      "value": "${{ github.workflow }}",
                      "short": true
                    },
                    {
                      "title": "Run Number",
                      "value": "${{ github.run_number }}",
                      "short": true
                    }
                  ],
                  "footer": "GitHub Actions",
                  "ts": '${{ github.event.head_commit.timestamp || github.event.repository.updated_at }}'
                }
              ]
            }'

      - name: Send failure notification to Slack
        if: failure() && env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -s -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H 'Content-Type: application/json' \
            -d '{
              "attachments": [
                {
                  "color": "#dc3545",
                  "title": "✗ Database Backup Failed",
                  "fields": [
                    {
                      "title": "Database",
                      "value": "${{ secrets.DB_NAME }}",
                      "short": true
                    },
                    {
                      "title": "Workflow",
                      "value": "${{ github.workflow }}",
                      "short": true
                    },
                    {
                      "title": "Run Number",
                      "value": "${{ github.run_number }}",
                      "short": true
                    },
                    {
                      "title": "Logs",
                      "value": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}",
                      "short": false
                    }
                  ],
                  "footer": "GitHub Actions"
                }
              ]
            }'

      - name: Send notification to Discord
        if: always() && env.DISCORD_WEBHOOK_URL != ''
        run: |
          STATUS="${{ job.status }}"
          COLOR="$([[ "${STATUS}" == "success" ]] && echo 5814783 || echo 16007990)"
          
          curl -s -X POST "${{ secrets.DISCORD_WEBHOOK_URL }}" \
            -H 'Content-Type: application/json' \
            -d "{
              \"embeds\": [
                {
                  \"title\": \"Database Backup ${STATUS^}\",
                  \"color\": ${COLOR},
                  \"fields\": [
                    {
                      \"name\": \"Database\",
                      \"value\": \"${{ secrets.DB_NAME }}\",
                      \"inline\": true
                    },
                    {
                      \"name\": \"Size\",
                      \"value\": \"${{ steps.backup.outputs.backup_size || 'N/A' }}\",
                      \"inline\": true
                    },
                    {
                      \"name\": \"Workflow\",
                      \"value\": \"${{ github.workflow }}\",
                      \"inline\": true
                    }
                  ],
                  \"url\": \"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\"
                }
              ]
            }"

      - name: Cleanup old artifacts
        if: github.event_name == 'schedule'
        run: |
          echo "Cleaning up old backup artifacts..."
          gh run list --workflow="${{ github.workflow }}" \
            --json databaseId,status,conclusion,createdAt \
            --jq '.[] | select(.status == "completed" and .conclusion == "success") | .databaseId' | \
            tail -n +11 | \
            while read id; do
              gh run delete "$id" 2>/dev/null || true
            done
        env:
          GH_TOKEN: ${{ github.token }}

  cleanup:
    name: Cleanup Old Backups
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Clean old S3 backups (optional)
        if: env.AWS_S3_BUCKET != ''
        run: |
          # Keep backups for last 30 days
          aws s3 ls s3://${{ secrets.AWS_S3_BUCKET }}/database-backups/ --recursive | \
            awk '{print $4, $1, $2}' | \
            while read key year month day time; do
              backup_date="${year}-${month}-${day}"
              backup_timestamp=$(date -d "${backup_date}" +%s 2>/dev/null || echo 0)
              cutoff_timestamp=$(date -d "30 days ago" +%s)
              
              if [[ ${backup_timestamp} -lt ${cutoff_timestamp} ]]; then
                echo "Deleting old backup: ${key}"
                aws sapi rm "s3://${{ secrets.AWS_S3_BUCKET }}/${key}" || true
              fi
            done
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_REGION: ${{ secrets.AWS_S3_REGION || 'us-east-1' }}
        continue-on-error: true
